{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "classes = [\"airplane\",\"automobile\",\"bird\",\"cat\",\"deer\",\"dog\",\"frog\",\"horse\",\"ship\",\"truck\"]\n",
    "\n",
    "def plot_sample(X, y, index):\n",
    "    plt.figure(figsize = (15,2))\n",
    "    plt.imshow(X[index])\n",
    "    plt.xlabel(classes[y[index]])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "IMG_SIZE=32\n",
    "NUM_CLASSES = 10\n",
    "TRAINING_BATCH_SIZE = 16\n",
    "TESTING_BATCH_SIZE = 100\n",
    "\n",
    "# AlexNet\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        print(\"Init:\")\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        \n",
    "        x = torch.randn(3, IMG_SIZE, IMG_SIZE).view(-1, 3, IMG_SIZE, IMG_SIZE)\n",
    "        self._to_linear = None\n",
    "        self.convs(x)\n",
    "        \n",
    "        print(x.shape)\n",
    "        print(self._to_linear)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self._to_linear, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 10),\n",
    "            nn.Softmax(dim=1),\n",
    "        )\n",
    "        \n",
    "    def convs(self, x):\n",
    "        # print(\"Conv\")\n",
    "        # print(x.shape)\n",
    "        assert x.ndim >= 2 and x.shape[1] == 3\n",
    "        \n",
    "        x = self.features(x)\n",
    "        \n",
    "        self._to_linear = x.flatten(start_dim=1).shape[1]\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # print(\"Forwarding:\")\n",
    "        # print(x.shape)\n",
    "        \n",
    "        x = self.convs(x)        \n",
    "        x = x.flatten(start_dim=1)\n",
    "\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "Net()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Init:\n",
      "torch.Size([1, 3, 32, 32])\n",
      "1152\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.9/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ../c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Net(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=1152, out_features=256, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): Linear(in_features=128, out_features=10, bias=True)\n",
       "    (5): Softmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Learning():\n",
    "    def __init__(self, model, optimizer, loss_fn, training_dataloader, testing_dataloader, transform_y, device, img_size: int, epochs: int):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.epochs = epochs\n",
    "        self.training_dataloader = training_dataloader\n",
    "        self.testing_dataloader = testing_dataloader\n",
    "        self.testing_batch = list(iter(testing_dataloader))\n",
    "        \n",
    "        self.transform_y = transform_y\n",
    "        self.img_size = img_size\n",
    "        \n",
    "        self.device = device\n",
    "        print(self.device)\n",
    "        self.log_path = f\"/logs/{int(time.time())}.csv\"\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            accs = []\n",
    "            losses = []\n",
    "            for i, (batch_X, batch_y) in enumerate(tqdm(iter(self.training_dataloader), desc=\"Epoch \" + str(epoch + 1))):\n",
    "                # print(\"Training:\")\n",
    "                # print(batch_y.shape)\n",
    "                batch_X = batch_X.to(dtype=torch.float32, device=self.device)\n",
    "                batch_y = batch_y.to(device=self.device)\n",
    "                \n",
    "                acc, loss = self._fwd_pass(batch_X, batch_y)\n",
    "                accs.append(acc)\n",
    "                losses.append(loss.to(device='cpu').detach().numpy())\n",
    "\n",
    "            acc = np.asarray(accs).mean()\n",
    "            loss = np.asarray(losses).mean()\n",
    "            print(\"Acc:\", acc)\n",
    "            print(\"Loss:\", loss)\n",
    "\n",
    "            val_acc, val_loss = self.test()\n",
    "            self.log_to_file(str(epoch), acc, loss, val_acc, val_loss)\n",
    "            print(\"Val Acc:\", val_acc)\n",
    "            print(\"Val Loss:\", val_loss)\n",
    "            \n",
    "    def overfit(self, epochs: int = 10, total_batches: int = 2):\n",
    "        iterator = iter(self.training_dataloader)\n",
    "        overfit_data = [next(iterator) for _ in range(total_batches)]\n",
    "        \n",
    "        accs = []\n",
    "        losses = []\n",
    "        for epoch in range(1,epochs + 1):\n",
    "            for i, (batch_X, batch_y) in enumerate(tqdm(overfit_data, total=total_batches, desc=\"Epoch \" + str(epoch))):\n",
    "                # print(\"Training:\")\n",
    "                batch_X = batch_X.to(dtype=torch.float32, device=self.device)\n",
    "                batch_y = batch_y.to(device=self.device)\n",
    "                \n",
    "                acc, loss = self._fwd_pass(batch_X, batch_y)\n",
    "            \n",
    "            accs.append(acc)\n",
    "            losses.append(loss.to(device='cpu').detach().numpy())\n",
    "            \n",
    "        val_acc, val_loss = self.test()\n",
    "        print(\"Val Acc:\", val_acc)\n",
    "        print(\"Val Loss:\", val_loss)\n",
    "        \n",
    "        plt.plot(range(1,epochs + 1), accs, range(1,epochs + 1), losses)\n",
    "        plt.gca().legend(('Accuracy','Loss'))\n",
    "        plt.ylabel(\"Training\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "    def test(self):\n",
    "        i = random.randint(0, len(self.testing_batch) - 1)\n",
    "        batch_X, batch_y = self.testing_batch[i]\n",
    "        \n",
    "        batch_X = batch_X.to(dtype=torch.float32, device=self.device).view(-1, 3, self.img_size, self.img_size)\n",
    "        batch_y = self.transform_y(batch_y).to(device=self.device)\n",
    "        \n",
    "        acc, loss = self._eval(batch_X, batch_y)\n",
    "\n",
    "        return acc, loss\n",
    "    \n",
    "\n",
    "    def _fwd_pass(self, X, y):\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        output = self.model(X)\n",
    "        y = self.transform_y(y)\n",
    "        \n",
    "        # print(y.shape)\n",
    "        # print(output.shape)\n",
    "        # print(y.dtype)\n",
    "        # print(output.dtype)\n",
    "        # print(output[0])\n",
    "        loss = self.loss_fn(output, y).to(device=self.device)\n",
    "\n",
    "        n_matches = torch.count_nonzero(torch.argmax(output, dim=1) == y).item()\n",
    "        \n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "            \n",
    "        return round(n_matches / len(X), 2), loss\n",
    "        \n",
    "    def _eval(self, X, y):\n",
    "        with torch.no_grad():\n",
    "            self.model.eval()\n",
    "                \n",
    "            output = self.model(X)\n",
    "            y = self.transform_y(y)\n",
    "\n",
    "            loss = self.loss_fn(output, y)\n",
    "\n",
    "            n_matches = torch.count_nonzero(torch.argmax(output, dim=1) == y).item()\n",
    "                \n",
    "            return round(n_matches / len(X), 2), loss\n",
    "    \n",
    "    def log_to_file(self, id: str, train_acc, train_loss, val_acc, val_loss):\n",
    "        append = f\"{id},{train_acc},{train_loss},{val_acc},{val_loss}\\n\"\n",
    "        full_path = os.path.abspath(os.getcwd()) + self.log_path\n",
    "        \n",
    "        with open(full_path, \"a\") as f:\n",
    "            if os.path.getsize(full_path) <= 0:\n",
    "                f.write(\"id,train_acc,train_loss,val_acc,val_loss\\n\")\n",
    "                \n",
    "            f.write(append)\n",
    "            "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "from torchvision import transforms\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "\n",
    "def find_mean_std(dataset):\n",
    "    assert dataset.ndim >= 2\n",
    "    print(dataset.shape)\n",
    "    \n",
    "    temp = dataset.reshape(dataset.shape[-1], -1)\n",
    "    \n",
    "    print(temp.shape)\n",
    "    \n",
    "    return np.array(temp.mean(axis=1)) / 255.0, np.array(temp.std(axis=1)) / 255.0\n",
    "    \n",
    "    \n",
    "training_norm = {\n",
    "    'mean': np.array([0.47410759, 0.4726623 , 0.47331911]), \n",
    "    'std': np.array([0.2520572 , 0.25201249, 0.25063239])\n",
    "}\n",
    "testing_norm = {\n",
    "    'mean': np.array([0.48003726, 0.47553964, 0.47417786]), \n",
    "    'std': np.array([0.25093406, 0.25146236, 0.25122434])\n",
    "}\n",
    "\n",
    "training_transforms = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.5),\n",
    "    transforms.RandomAdjustSharpness(0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=training_norm['mean'], std=training_norm['std'])\n",
    "])\n",
    "\n",
    "testing_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=testing_norm['mean'], std=testing_norm['std'])\n",
    "])\n",
    "\n",
    "NUM_AUG = 2\n",
    "training_dataset = torch.utils.data.ConcatDataset([torchvision.datasets.CIFAR10(root=\"./data\", train=True, transform=training_transforms, download=True) for _ in range(NUM_AUG)])\n",
    "training_dataloader = torch.utils.data.DataLoader(training_dataset, shuffle=True, batch_size=TRAINING_BATCH_SIZE)\n",
    "\n",
    "testing_dataset = torchvision.datasets.CIFAR10(root=\"./data\", train=False, transform=testing_transforms, download=True)\n",
    "testing_dataloader = torch.utils.data.DataLoader(testing_dataset, \n",
    "                                                 shuffle=True, \n",
    "                                                 batch_size=TESTING_BATCH_SIZE)\n",
    "\n",
    "\n",
    "index = 5\n",
    "dataset = testing_dataset\n",
    "plt.figure(figsize = (15,2))\n",
    "# print(dataset[index][0].shape)\n",
    "#.view(dataset[index][0].shape[1], dataset[index][0].shape[2], dataset[index][0].shape[0])\n",
    "plt.imshow(dataset[index][0].permute(1, 2, 0))\n",
    "plt.xlabel(classes[dataset[index][1]])\n",
    "\n",
    "print(len(training_dataloader) * TRAINING_BATCH_SIZE)\n",
    "# print(testing_dataset[0][0] /255)\n",
    "\n",
    "# print(find_mean_std(training_dataset.data))\n",
    "# print(find_mean_std(testing_dataset.data))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "100000\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "to_overfit = False\n",
    "\n",
    "net = Net().to(device=device)\n",
    "\n",
    "# net = torchvision.models.DenseNet(\n",
    "#     growth_rate = 16, # how many filters to add each layer (`k` in paper)\n",
    "#     block_config = (6, 12, 24, 16), # how many layers in each pooling block\n",
    "#     num_init_features = 16, # the number of filters to learn in the first convolution layer (k0)\n",
    "#     bn_size= 4, # multiplicative factor for number of bottleneck (1x1 cons) layers\n",
    "#     drop_rate = 0, # dropout rate after each dense conv layer\n",
    "#     num_classes = NUM_CLASSES # number of classification classes\n",
    "# ).to(device=device)\n",
    "\n",
    "net = torchvision.models.resnet18(\n",
    "    pretrained=True,\n",
    ")\n",
    "\n",
    "#Freeze params\n",
    "for name, param in net.named_parameters(): \n",
    "    if(\"bn\" not in name):\n",
    "        param.requires_grad = False\n",
    "        \n",
    "net.fc = nn.Sequential(\n",
    "    nn.Linear(net.fc.in_features,500),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.0 if to_overfit else 0.1), \n",
    "    nn.Linear(500, NUM_CLASSES)\n",
    ")\n",
    "\n",
    "net = net.to(device=device)\n",
    "\n",
    "learning = Learning(\n",
    "    model=net,\n",
    "    optimizer=optim.Adam(net.parameters(), lr=3e-4, weight_decay=0),\n",
    "    # loss_fn=nn.MSELoss(),\n",
    "    loss_fn=nn.CrossEntropyLoss(),\n",
    "    img_size=IMG_SIZE,\n",
    "    training_dataloader=training_dataloader,\n",
    "    testing_dataloader=testing_dataloader,\n",
    "    device=device,\n",
    "    # transform_y=lambda y: F.one_hot(y.long(), num_classes=NUM_CLASSES).to(dtype=torch.float32),\n",
    "    transform_y=lambda y: y,\n",
    "    epochs=5,\n",
    ")\n",
    "\n",
    "if to_overfit:\n",
    "  learning.overfit(epochs=10, total_batches=1)\n",
    "else:\n",
    "  learning.train()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Init:\n",
      "torch.Size([1, 3, 32, 32])\n",
      "1152\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 1:   0%|          | 1/6250 [00:00<10:56,  9.51it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cpu\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 1:   8%|â–Š         | 500/6250 [00:58<12:00,  7.98it/s]"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Acc: 0.808212\n",
    "# Loss: 0.5360819\n",
    "# Val Acc: 0.72\n",
    "# Val Loss: tensor(0.8811, device='cuda:0')\n",
    "torch.save(net.state_dict(), 'image_classification_resnet18.pth')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit"
  },
  "interpreter": {
   "hash": "4cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}